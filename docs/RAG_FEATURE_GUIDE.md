# ğŸ¤– RAGåŠŸèƒ½ä½¿ç”¨æŒ‡å—

## ğŸ“– æ¦‚è¿°

RAGï¼ˆRetrieval-Augmented Generationï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰åŠŸèƒ½å·²æˆåŠŸé›†æˆåˆ°æœç´¢ç³»ç»Ÿä¸­ã€‚å½“ç”¨æˆ·é€‰æ‹©TF-IDFæ’åºæ¨¡å¼æ—¶ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨ç”ŸæˆåŸºäºæ£€ç´¢ç»“æœçš„æ™ºèƒ½å›ç­”ã€‚

## ğŸ¯ åŠŸèƒ½ç‰¹ç‚¹

### âœ… å·²å®ç°åŠŸèƒ½

1. **æ¡ä»¶å¯ç”¨** - ä»…åœ¨TF-IDFæ¨¡å¼ä¸‹å¯ç”¨RAGåŠŸèƒ½
2. **æ™ºèƒ½å›ç­”** - åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£ç”Ÿæˆå‡†ç¡®å›ç­”
3. **ç¼“å­˜æœºåˆ¶** - æ”¯æŒå›ç­”ç¼“å­˜ï¼Œæé«˜å“åº”é€Ÿåº¦
4. **é”™è¯¯å¤„ç†** - å®Œå–„çš„å¼‚å¸¸å¤„ç†å’Œé™çº§æœºåˆ¶
5. **é…ç½®çµæ´»** - æ”¯æŒå¤šç§é…ç½®å‚æ•°è°ƒæ•´

### ğŸ”§ æŠ€æœ¯æ¶æ„

```mermaid
flowchart TD
    A[ç”¨æˆ·æŸ¥è¯¢] --> B[IndexService.retrieve]
    B --> C[å¬å›æ–‡æ¡£IDåˆ—è¡¨]
    C --> D[IndexService.rank]
    D --> E{æ’åºæ¨¡å¼}
    E -->|tfidf| F[TF-IDFæ’åº]
    E -->|ctr| G[CTRæ’åº]
    F --> H[RAGå¢å¼ºå¤„ç†]
    G --> I[è¿”å›ç»“æœ]
    H --> I
```

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### 1. å¯åŠ¨ç³»ç»Ÿ

```bash
python start_system.py
```

### 2. ä½¿ç”¨RAGåŠŸèƒ½

1. **è¿›å…¥æœç´¢æ ‡ç­¾é¡µ** - ç‚¹å‡»"ğŸ” ç¬¬äºŒéƒ¨åˆ†ï¼šåœ¨çº¿å¬å›æ’åº"
2. **é€‰æ‹©æ’åºæ¨¡å¼** - åœ¨"æ’åºç®—æ³•"ä¸‹æ‹‰èœå•ä¸­é€‰æ‹©"tfidf"
3. **è¾“å…¥æŸ¥è¯¢** - åœ¨"å®éªŒæŸ¥è¯¢"æ¡†ä¸­è¾“å…¥é—®é¢˜
4. **æ‰§è¡Œæœç´¢** - ç‚¹å‡»"ğŸ”¬ æ‰§è¡Œæ£€ç´¢"æŒ‰é’®
5. **æŸ¥çœ‹RAGå›ç­”** - åœ¨"ğŸ¤– RAGæ™ºèƒ½å›ç­”"åŒºåŸŸæŸ¥çœ‹ç”Ÿæˆçš„å›ç­”

### 3. ç¤ºä¾‹æŸ¥è¯¢

æ¨èæµ‹è¯•æŸ¥è¯¢ï¼š
- "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"
- "æœºå™¨å­¦ä¹ çš„åŸºæœ¬åŸç†"
- "æ·±åº¦å­¦ä¹ çš„ç‰¹ç‚¹"
- "è‡ªç„¶è¯­è¨€å¤„ç†"

## âš™ï¸ é…ç½®å‚æ•°

### RAGConfig é…ç½®ç±»

```python
@dataclass
class RAGConfig:
    enabled: bool = True                    # æ˜¯å¦å¯ç”¨RAGåŠŸèƒ½
    llm_provider: str = "mock"             # LLMæä¾›å•† (mock, openai, local)
    model_name: str = "gpt-3.5-turbo"      # æ¨¡å‹åç§°
    max_context_tokens: int = 3000         # æœ€å¤§ä¸Šä¸‹æ–‡tokenæ•°
    top_k_docs: int = 3                    # ä½¿ç”¨å‰Kä¸ªæ–‡æ¡£
    temperature: float = 0.7               # ç”Ÿæˆæ¸©åº¦
    max_response_tokens: int = 500         # æœ€å¤§å›ç­”tokenæ•°
    cache_enabled: bool = True             # æ˜¯å¦å¯ç”¨ç¼“å­˜
    cache_ttl: int = 3600                  # ç¼“å­˜è¿‡æœŸæ—¶é—´(ç§’)
```

### è‡ªå®šä¹‰é…ç½®ç¤ºä¾‹

```python
from search_engine.rag_service import RAGConfig, RAGService

# åˆ›å»ºè‡ªå®šä¹‰é…ç½®
config = RAGConfig(
    enabled=True,
    llm_provider="mock",
    top_k_docs=5,
    max_context_tokens=2000,
    cache_enabled=True,
    cache_ttl=1800  # 30åˆ†é’Ÿç¼“å­˜
)

# åˆ›å»ºRAGæœåŠ¡
rag_service = RAGService(config, index_service)
```

## ğŸ” æ ¸å¿ƒç»„ä»¶

### 1. RAGService ç±»

**ä¸»è¦æ–¹æ³•**ï¼š
- `enhance_search_results()` - åŸºäºæœç´¢ç»“æœç”ŸæˆRAGå›ç­”
- `build_context()` - æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
- `generate_answer()` - ç”Ÿæˆå›ç­”
- `get_stats()` - è·å–æœåŠ¡ç»Ÿè®¡ä¿¡æ¯

### 2. MockLLMClient ç±»

**åŠŸèƒ½**ï¼š
- æ¨¡æ‹ŸLLMå®¢æˆ·ç«¯ï¼Œç”¨äºæµ‹è¯•
- æ”¯æŒå¸¸è§æŸ¥è¯¢çš„æ¨¡æ¿å›ç­”
- æ— éœ€å¤–éƒ¨APIä¾èµ–

### 3. ç¼“å­˜æœºåˆ¶

**ç‰¹ç‚¹**ï¼š
- åŸºäºæŸ¥è¯¢å’Œæ–‡æ¡£IDçš„MD5å“ˆå¸Œç¼“å­˜é”®
- æ”¯æŒTTLè¿‡æœŸæ—¶é—´
- è‡ªåŠ¨æ¸…ç†è¿‡æœŸç¼“å­˜

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### 1. ä¸Šä¸‹æ–‡æ„å»ºä¼˜åŒ–

```python
def build_context(self, search_results: List[Tuple], max_tokens: int = None) -> str:
    """æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯"""
    max_tokens = max_tokens or self.config.max_context_tokens
    context_parts = []
    current_tokens = 0
    
    for i, result in enumerate(search_results):
        if len(result) >= 3:
            doc_id, score, summary = result[0], result[1], result[2]
        else:
            continue
        
        # è·å–å®Œæ•´æ–‡æ¡£å†…å®¹
        if self.index_service:
            full_content = self.index_service.get_document(doc_id)
            if not full_content:
                full_content = summary  # é™çº§ä½¿ç”¨æ‘˜è¦
        else:
            full_content = summary
        
        # è®¡ç®—tokenæ•°é‡ï¼ˆç®€åŒ–ä¼°ç®—ï¼š1ä¸ªtokençº¦ç­‰äº4ä¸ªå­—ç¬¦ï¼‰
        estimated_tokens = len(full_content) // 4
        
        if current_tokens + estimated_tokens <= max_tokens:
            context_parts.append(f"æ–‡æ¡£{i+1} (ID: {doc_id}, ç›¸å…³åº¦: {score:.4f}):\n{full_content}\n")
            current_tokens += estimated_tokens
        else:
            # å¦‚æœè¶…å‡ºtokené™åˆ¶ï¼Œæˆªå–éƒ¨åˆ†å†…å®¹
            remaining_tokens = max_tokens - current_tokens
            if remaining_tokens > 100:  # è‡³å°‘ä¿ç•™100ä¸ªtoken
                truncated_content = full_content[:remaining_tokens * 4] + "..."
                context_parts.append(f"æ–‡æ¡£{i+1} (ID: {doc_id}, ç›¸å…³åº¦: {score:.4f}):\n{truncated_content}\n")
            break
    
    return "\n".join(context_parts)
```

### 2. ç¼“å­˜ä¼˜åŒ–

```python
def _get_cache_key(self, query: str, search_results: List[Tuple]) -> str:
    """ç”Ÿæˆç¼“å­˜é”®"""
    # åŸºäºæŸ¥è¯¢å’Œæ–‡æ¡£IDç”Ÿæˆç¼“å­˜é”®
    doc_ids = [str(result[0]) for result in search_results if len(result) > 0]
    cache_content = f"{query}_{'_'.join(sorted(doc_ids))}"
    return hashlib.md5(cache_content.encode()).hexdigest()
```

## ğŸ§ª æµ‹è¯•éªŒè¯

### 1. åŠŸèƒ½æµ‹è¯•

```bash
# è¿è¡ŒRAGåŠŸèƒ½æµ‹è¯•
python test_rag_functionality.py

# è¿è¡ŒRAGé›†æˆæµ‹è¯•
python test_rag_integration.py
```

### 2. æµ‹è¯•è¦†ç›–

- âœ… æ¨¡æ‹ŸLLMå®¢æˆ·ç«¯æµ‹è¯•
- âœ… RAGæœåŠ¡åŸºæœ¬åŠŸèƒ½æµ‹è¯•
- âœ… ä¸ç´¢å¼•æœåŠ¡é›†æˆæµ‹è¯•
- âœ… ç¼“å­˜åŠŸèƒ½æµ‹è¯•
- âœ… ä¸åŒé…ç½®å‚æ•°æµ‹è¯•

## ğŸ”§ æ‰©å±•å¼€å‘

### 1. é›†æˆçœŸå®LLM

```python
# åœ¨RAGService._init_llm_client()ä¸­æ·»åŠ 
elif self.config.llm_provider == "openai":
    import openai
    openai.api_key = os.getenv("OPENAI_API_KEY")
    return openai
```

### 2. æ·»åŠ æ–°çš„LLMæä¾›å•†

```python
elif self.config.llm_provider == "local":
    # é›†æˆæœ¬åœ°LLMæ¨¡å‹
    from local_llm import LocalLLMClient
    return LocalLLMClient(self.config.model_name)
```

### 3. ä¼˜åŒ–æç¤ºè¯

```python
def generate_answer(self, query: str, context: str) -> str:
    """ç”Ÿæˆå›ç­”"""
    prompt = f"""
åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜: {query}

æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£:
{context}

è¯·åŸºäºä¸Šè¿°æ–‡æ¡£å†…å®¹ï¼Œç”Ÿæˆä¸€ä¸ªå‡†ç¡®ã€å…¨é¢çš„å›ç­”ã€‚è¦æ±‚ï¼š
1. å›ç­”è¦å‡†ç¡®ï¼ŒåŸºäºæ–‡æ¡£å†…å®¹
2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜
3. å›ç­”è¦ç®€æ´æ˜äº†ï¼Œçªå‡ºé‡ç‚¹
4. å¯ä»¥å¼•ç”¨å…·ä½“çš„æ–‡æ¡£ä¿¡æ¯

å›ç­”:
"""
    # è°ƒç”¨LLMç”Ÿæˆå›ç­”
    response = self.llm_client.generate(prompt, self.config.temperature)
    return response.strip()
```

## ğŸ“ˆ ç›‘æ§å’Œç»Ÿè®¡

### 1. æœåŠ¡ç»Ÿè®¡

```python
stats = rag_service.get_stats()
print(f"RAGæœåŠ¡ç»Ÿè®¡: {stats}")
```

**ç»Ÿè®¡ä¿¡æ¯åŒ…æ‹¬**ï¼š
- åŠŸèƒ½å¯ç”¨çŠ¶æ€
- LLMæä¾›å•†ä¿¡æ¯
- ç¼“å­˜çŠ¶æ€å’Œå¤§å°
- é…ç½®å‚æ•°

### 2. æ€§èƒ½ç›‘æ§

```python
# ç›‘æ§RAGå¤„ç†æ—¶é—´
import time

start_time = time.time()
rag_answer = rag_service.enhance_search_results(query, search_results)
processing_time = time.time() - start_time

print(f"RAGå¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
```

## ğŸš¨ æ•…éšœæ’é™¤

### 1. å¸¸è§é—®é¢˜

**é—®é¢˜1**: RAGåŠŸèƒ½ä¸æ˜¾ç¤º
- **è§£å†³æ–¹æ¡ˆ**: ç¡®ä¿é€‰æ‹©äº†"tfidf"æ’åºæ¨¡å¼

**é—®é¢˜2**: å›ç­”è´¨é‡ä¸ä½³
- **è§£å†³æ–¹æ¡ˆ**: è°ƒæ•´`top_k_docs`å’Œ`max_context_tokens`å‚æ•°

**é—®é¢˜3**: å“åº”é€Ÿåº¦æ…¢
- **è§£å†³æ–¹æ¡ˆ**: å¯ç”¨ç¼“å­˜ï¼Œå‡å°‘`top_k_docs`æ•°é‡

### 2. é”™è¯¯å¤„ç†

```python
try:
    rag_answer = rag_service.enhance_search_results(query, search_results)
except Exception as e:
    print(f"RAGå¤„ç†å¤±è´¥: {e}")
    rag_answer = f"RAGåŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨: {str(e)}"
```

## ğŸ¯ æœ€ä½³å®è·µ

### 1. é…ç½®å»ºè®®

- **ç”Ÿäº§ç¯å¢ƒ**: ä½¿ç”¨çœŸå®çš„LLMæä¾›å•†
- **å¼€å‘æµ‹è¯•**: ä½¿ç”¨MockLLMClient
- **ç¼“å­˜è®¾ç½®**: æ ¹æ®æŸ¥è¯¢é¢‘ç‡è°ƒæ•´TTL
- **æ–‡æ¡£æ•°é‡**: æ ¹æ®æ–‡æ¡£é•¿åº¦è°ƒæ•´top_k_docs

### 2. æ€§èƒ½ä¼˜åŒ–

- åˆç†è®¾ç½®ä¸Šä¸‹æ–‡tokené™åˆ¶
- å¯ç”¨ç¼“å­˜æœºåˆ¶
- ä¼˜åŒ–æ–‡æ¡£æ£€ç´¢è´¨é‡
- ç›‘æ§å¤„ç†æ—¶é—´

### 3. ç”¨æˆ·ä½“éªŒ

- æä¾›æ¸…æ™°çš„é”™è¯¯æç¤º
- æ”¯æŒé™çº§æœºåˆ¶
- ä¼˜åŒ–å›ç­”æ ¼å¼
- ä¿æŒç•Œé¢ä¸€è‡´æ€§

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [é¡¹ç›®æ¶æ„æ–‡æ¡£](./ARCHITECTURE_AND_MODULES.md)
- [APIæ–‡æ¡£](./API.md)
- [å¿«é€Ÿå¼€å§‹æŒ‡å—](./QUICK_START.md)
- [å®‰è£…è¯´æ˜](./INSTALLATION.md)

---

**ç‰ˆæœ¬**: 1.0.0  
**æ›´æ–°æ—¥æœŸ**: 2024-01-15  
**ç»´æŠ¤è€…**: å¼€å‘å›¢é˜Ÿ 